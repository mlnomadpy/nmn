<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Learning 2.0: Artificial Neurons that Matter</title>
    <style>
        /* Basic Reset */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f4f4f9;
            padding: 20px;
            max-width: 800px;
            margin: auto;
        }
        header {
            text-align: center;
            margin-bottom: 40px;
        }
        h1 {
            font-size: 2em;
            color: #2c3e50;
            margin-bottom: 10px;
        }
        p {
            margin-bottom: 1em;
            color: #555;
        }
        /* Author List Style */
        header p {
            font-size: 1em;
            color: #777;
            font-style: italic;
        }
        /* Section Titles */
        h2 {
            font-size: 1.5em;
            color: #34495e;
            margin-top: 20px;
            border-bottom: 2px solid #ddd;
            padding-bottom: 5px;
        }
        /* Abstract and Acknowledgments Sections */
        section p {
            margin-bottom: 1em;
            line-height: 1.7;
        }
        /* Demo Links */
        ul {
            list-style-type: none;
            padding: 0;
        }
        li {
            margin: 8px 0;
        }
        a {
            text-decoration: none;
            color: #3498db;
            font-weight: bold;
            transition: color 0.2s;
        }
        a:hover {
            color: #1abc9c;
        }
        /* Acknowledgments Section */
        #acknowledgments {
            background-color: #e8f6f3;
            padding: 15px;
            border-radius: 5px;
            border-left: 4px solid #1abc9c;
            margin-top: 20px;
        }
        /* Button Container Styles */
        .button-container {
            margin: 20px 0;
            display: flex;
            justify-content: center;
            gap: 15px;
        }
        
        .action-button {
            display: inline-flex;
            align-items: center;
            padding: 8px 16px;
            border-radius: 20px;
            background-color: #2c3e50;
            color: white;
            text-decoration: none;
            font-size: 0.9em;
            transition: all 0.3s ease;
            border: none;
            cursor: pointer;
        }

        .action-button:hover {
            background-color: #34495e;
            transform: translateY(-2px);
        }

        .action-button img {
            width: 20px;
            height: 20px;
            margin-right: 8px;
        }

        .action-button.kaggle {
            background-color: #20BEFF;
        }
        .action-button.kaggle:hover {
            background-color: #1a9bd1;
        }

        .action-button.arxiv {
            background-color: #B31B1B;
        }
        .action-button.arxiv:hover {
            background-color: #8B1414;
        }

        /* Citation Section Styles */
        .citation-box {
            background-color: #f8f9fa;
            padding: 15px;
            border-radius: 5px;
            border: 1px solid #dee2e6;
            margin: 10px 0;
            font-family: monospace;
            white-space: pre-wrap;
            word-wrap: break-word;
        }

        .copy-button {
            padding: 5px 10px;
            background-color: #2c3e50;
            color: white;
            border: none;
            border-radius: 3px;
            cursor: pointer;
            float: right;
        }

        .copy-button:hover {
            background-color: #34495e;
        }
    </style>
</head>
<body>
    <!-- Main Title and Author List -->
    <header>
        <h1>Deep Learning 2.0: Artificial Neurons That Matter - Reject Correlation, Embrace Orthogonality</h1>
        <p><strong>Authors:</strong> Taha Bouhsine</p>
        <div class="button-container">
            <a href="https://github.com/mlnomadpy/nmn" class="action-button">
                <img src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png" alt="GitHub">
                GitHub Repository
            </a>
            <a href="#" class="action-button">
                <img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1611902473383/CDyAuTy75.png" alt="Blog">
                Read Blog Post
            </a>
            <a href="https://github.com/sponsors/mlnomadpy" class="action-button">
                <img src="https://github.githubassets.com/images/modules/site/sponsors/heart-explosion.png" alt="Sponsor">
                Sponsor Project
            </a>
            <a href="https://www.kaggle.com/code/yournotebook" class="action-button kaggle">
                <img src="https://www.kaggle.com/static/images/site-logo.svg" alt="Kaggle">
                Kaggle Notebook
            </a>
            <a href="https://arxiv.org/abs/yourpaper" class="action-button arxiv">
                <img src="https://static.arxiv.org/static/browse/0.3.2.8/images/arxiv-logo-one-color-white.svg" alt="arXiv">
                Read Paper
            </a>
        </div>
    </header>

    <!-- Abstract Section -->
    <section>
        <h2>Abstract</h2>
        <p>
            We introduce the ⵟ-Perceptron Layer, known as the Neural Matter Layer (NML), a breakthrough in neural network architecture that achieves non-linear pattern recognition without activation functions. Our key innovation, the 
            ⵟ-product and posi-ⵟ-product, naturally induces non-linearity by projecting inputs into a semi-metric space, eliminating the need for traditional activation functions while maintaining only a softmax layer for final class probability distribution. This approach simplifies network architecture and provides unprecedented transparency into the network's decision-making process.
        </p>
        <p>
            Our comprehensive empirical evaluation across different datasets demonstrates that ⵟ-MLP consistently outperforms traditional MLPs, both with and without activation functions. The results challenge the fundamental assumption that separate activation functions are necessary for effective deep-learning models. Beyond performance improvements, our method offers significant advantages in computational efficiency and model interpretability.
        </p>
        <p>
            The implications of this work extend beyond immediate architectural benefits: by eliminating intermediate activation functions while preserving non-linear capabilities, ⵟ-MLP establishes a new paradigm for neural network design that combines simplicity with effectiveness. Most importantly, our approach provides unprecedented insights into the traditionally opaque "black-box" nature of neural networks, offering a clearer understanding of how these models process and classify information.
        </p>
    </section>

    <!-- Demos Section -->
    <section>
        <h2>Demos and Tools</h2>
        <ul>
            <li><a href="./single_neuron.html" title="Explore a single artificial neuron">Single Neuron in Your Area</a></li>
            <li><a href="./neuron_space.html" title="Explore neuron space visualization">Neurons in Your Space</a></li>
            <li><a href="./do_you_even_mnist.html" title="View the MNIST dataset demonstration">Do You Even MNIST, Bro?</a></li>
            <li><a href="./classify_your_own.html" title="View the Classify Your Own Dataset">NNMap and CYOD: Classify your Own Dataset</a></li>

        </ul>
    </section>

    <section>
        <h2>Blog Posts</h2>
        <ul>
            <li><a href="./single_neuron.html" title="What is missing in Deep Learning and why is it a black box?">A Topological Tale of Everything Wrong with Deep Learning</a></li>
        </ul>
    </section>

    <!-- Acknowledgments Section -->
    <section id="acknowledgments">
        <h2>Acknowledgments</h2>
        <p>
            The Google Developer Expert program and Google AI/ML Developer Programs team supported this work by providing Google Cloud Credit. I want to extend my gratitude to the staff at High Grounds Coffee Roasters for their excellent coffee and peaceful atmosphere. I would also like to thank Dr. Andrew Ng for creating the Deep Learning course that introduced me to this field, without his efforts to democratize access to knowledge, this work would not have been possible. Additionally, I want to express my appreciation to all the communities I have been part of, especially MLNomads, Google Developers, and MLCollective communities        </p>
    </section>

    <!-- Citation Section -->
    <section>
        <h2>Cite This Work</h2>
        <p>If you use this work in your research, please cite:</p>
        
        <h3>BibTeX</h3>
        <div class="citation-box">
            <button class="copy-button" onclick="navigator.clipboard.writeText(this.nextElementSibling.textContent)">Copy</button>
            <code>
@article{bouhsine2024deep,
    title={Deep Learning 2.0: Artificial Neurons That Matter - Reject Correlation, Embrace Orthogonality},
    author={Bouhsine, Taha},
    journal={arXiv preprint arXiv:2024.xxxxx},
    year={2024}
}
</code>
        </div>

        <h3>Plain Text Citation</h3>
        <div class="citation-box">
            <button class="copy-button" onclick="navigator.clipboard.writeText(this.nextElementSibling.textContent)">Copy</button>
            <code>
Bouhsine, T. (2024). Deep Learning 2.0: Artificial Neurons That Matter - Reject Correlation, Embrace Orthogonality. arXiv preprint arXiv:2024.xxxxx.</code>
        </div>
    </section>

    <section id="license">
        <h2>License</h2>
        <p>
            The source code, algorithms, and all contributions presented in this work are licensed under the GNU Affero General Public License (AGPL) v3.0. This license ensures that any use, modification, or distribution of the code and any adaptations or applications of the underlying models and methods must be made publicly available under the same license. This applies whether the work is used for personal, academic, or commercial purposes, including services provided over a network.
        </p>
    </section>

</body>
</html>
